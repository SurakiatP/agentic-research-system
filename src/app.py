import gradio as gr
import uuid
import time
from typing import List, Dict, Any, Optional
from src.core.graph import graph_app
from src.utils.logger import logger
from src.core.state import AgentState, SubQuery
from src.utils import observability

# Langfuse auto-tracing via LangChain CallbackHandler (SDK v3)
try:
    from langfuse.langchain import CallbackHandler as LangfuseCallbackHandler
    HAS_LANGFUSE = True
    logger.info("Langfuse CallbackHandler available.")
except ImportError:
    HAS_LANGFUSE = False
    logger.info("Langfuse CallbackHandler not available.")


def _make_config(thread_id: str, trace_id: Optional[str] = None) -> dict:
    """
    Build config dict with thread_id and Langfuse callbacks.
    Attributes:
        trace_id: Explicit Trace ID for linking scores. If None, auto-generated by handler.
    """
    cfg = {
        "configurable": {"thread_id": thread_id},
        "recursion_limit": 50
    }
    
    if HAS_LANGFUSE and trace_id:
        # SDK only accepts: public_key, update_trace, trace_context
        # Use trace_context to link a custom trace_id
        handler = LangfuseCallbackHandler(
            trace_context={"trace_id": trace_id},
            update_trace=True,
        )
        cfg["callbacks"] = [handler]
        # Pass session_id via LangChain metadata (SDK reads "langfuse_session_id")
        cfg.setdefault("metadata", {})["langfuse_session_id"] = thread_id
        
    return cfg


# === Agent Label Constants ===
LABEL_CACHE = "üîç Semantic Cache Agent"
LABEL_ROUTER = "üß≠ Router Agent"
LABEL_GENERAL_CHAT = "üí¨ General Chat"
LABEL_MANAGER = "üß† Manager Agent"
LABEL_RESEARCHER = "üîé Researcher Agent"
LABEL_SUMMARIZER = "üìù Summarize Answer"


def _msg(label: str, content: str) -> Dict[str, Any]:
    """Create a labeled agent message with metadata title."""
    return {"role": "assistant", "content": content, "metadata": {"title": label}}


def generate_plan(query: str, history: List[Dict], thread_id: str):
    """
    Step 1: Run Cache Check ‚Üí Router ‚Üí (General Chat ‚Üí END) or (Manager ‚Üí HITL #1 interrupt).
    """
    start_time = time.perf_counter()
    
    if not query:
        yield history, gr.update(visible=False), gr.update(visible=False), thread_id, []
        return

    if not thread_id:
        thread_id = str(uuid.uuid4())
        
    # Generate Trace ID for this execution turn (Must be 32-char hex, no dashes)
    current_trace_id = uuid.uuid4().hex

    logger.info(f"Starting planning for thread {thread_id} (Trace: {current_trace_id})")

    config = _make_config(thread_id, trace_id=current_trace_id)
    initial_state = {
        "user_query": query,
        "loop_count": 0,
        "reflection_count": 0
    }

    # Step 1: User message + cache checking status
    new_history = history + [
        {"role": "user", "content": query},
        _msg(LABEL_CACHE, "Searching cache...")
    ]
    yield new_history, gr.update(visible=False), gr.update(visible=False), thread_id, []

    # Metrics Collection
    cache_hit = False
    tools_called = []
    
    try:
        final_answer = None
        route_decision = None
        
        for event in graph_app.stream(initial_state, config=config):
            if "cache_check" in event:
                data = event["cache_check"]
                cache_hit = data.get("cache_hit", False)
                if cache_hit:
                    final_answer = data.get("final_answer", "")
                    
            if "router" in event:
                route_decision = event["router"].get("route_decision", "deep_research")

            if "general_chat" in event:
                final_answer = event["general_chat"].get("final_answer", "")

            if "manager" in event:
                pass  # Manager finished planning

        # Check graph state
        snapshot = graph_app.get_state(config)
        
        # === Calculate Metrics & Log Scores ===
        elapsed = time.perf_counter() - start_time
        
        # Estimate Model Cost (Heuristic based on final answer length?)
        # Since we can't easily get token count from stream, we approximate or rely on Langfuse native
        # But we log a placeholder for cost_agent_llm to satisfy user request?
        # Better: Estimate based on final_answer len if available
        est_output_tokens = len(final_answer) / 4 if final_answer else 0
        est_input_tokens = len(query) / 4
        model_cost = observability.estimate_model_cost(est_input_tokens, est_output_tokens)
        
        scores = observability.calculate_production_scores(
            elapsed_sec=elapsed,
            cache_hit=cache_hit,
            tools_called=tools_called, # Tools not usually called in planning phase (except Cache)
            model_cost=model_cost
        )
        
        # Async Log to Langfuse
        if HAS_LANGFUSE:
            observability.log_scores_to_langfuse(current_trace_id, scores)
        
        # === Path A: Cache Hit ‚Üí Show cached answer ===
        if final_answer and snapshot.values.get("cache_hit"):
            new_history = history + [
                {"role": "user", "content": query},
                _msg(LABEL_CACHE, f"‚ö° **Cache Hit!** Found a previous answer:\n\n{final_answer}")
            ]
            yield new_history, gr.update(visible=False), gr.update(visible=False), thread_id, []
            return

        # === Path B: General Chat ‚Üí Show answer directly ===
        if route_decision == "general_chat" and final_answer:
            new_history = history + [
                {"role": "user", "content": query},
                _msg(LABEL_CACHE, "‚ùå Cache Miss ‚Äî No cached answer found."),
                _msg(LABEL_ROUTER, "üß≠ Route: **General Chat** (no deep research needed)"),
                _msg(LABEL_GENERAL_CHAT, final_answer)
            ]
            yield new_history, gr.update(visible=False), gr.update(visible=False), thread_id, []
            return

        # === Path C: Deep Research ‚Üí Show sub-queries for HITL #1 ===
        if snapshot.next:
            sub_queries = snapshot.values.get("sub_queries", [])
            
            sq_list = "\n".join([f"- {sq.sub_query}" for sq in sub_queries])
            choices = [sq.sub_query for sq in sub_queries]
            
            new_history = history + [
                {"role": "user", "content": query},
                _msg(LABEL_CACHE, "‚ùå Cache Miss ‚Äî No cached answer found."),
                _msg(LABEL_ROUTER, "üß≠ Route: **Deep Research** (complex query detected)"),
                _msg(LABEL_MANAGER, f"Analysis complete. Generated **{len(sub_queries)} sub-queries:**\n\n{sq_list}\n\nüëá Select the sub-queries you want to research, then click **Approve**.")
            ]
            yield (
                new_history,
                gr.update(choices=choices, value=choices, visible=True, interactive=True),
                gr.update(visible=True),
                thread_id,
                sub_queries
            )
        else:
            new_history = history + [
                {"role": "user", "content": query},
                _msg(LABEL_CACHE, "‚ùå Error: Graph finished unexpectedly without a plan.")
            ]
            yield new_history, gr.update(visible=False), gr.update(visible=False), thread_id, []

    except Exception as e:
        logger.error(f"Planning failed: {e}")
        new_history = history + [
            {"role": "user", "content": query},
            _msg(LABEL_CACHE, f"‚ùå Error: {str(e)}")
        ]
        yield new_history, gr.update(visible=False), gr.update(visible=False), thread_id, []


def execute_research(selected_queries: List[str], history: List[Dict], thread_id: str, original_sqs: List[SubQuery]):
    """
    Step 2: Filter sub-queries ‚Üí Resume Graph ‚Üí Researcher ‚Üí Summarize Answer ‚Üí HITL #2
    """
    start_time = time.perf_counter()
    current_trace_id = uuid.uuid4().hex  # Must be 32-char hex, no dashes
    
    logger.info(f"Resuming research for thread {thread_id} (Trace: {current_trace_id})")

    config = _make_config(thread_id, trace_id=current_trace_id)

    if not selected_queries:
        yield (
            history + [_msg(LABEL_RESEARCHER, "‚ùå Error: Please select at least 1 sub-query.")],
            gr.update(visible=True),
            gr.update(visible=True),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False)
        )
        return

    # Build updated sub-query list
    updated_sqs = []
    for sq in original_sqs:
        if sq.sub_query in selected_queries:
            sq.status = "pending"
            updated_sqs.append(sq)

    graph_app.update_state(config, {"sub_queries": updated_sqs})

    # Show "researching" status
    sq_progress = "\n".join([f"- ‚è≥ {sq.sub_query}" for sq in updated_sqs])
    researcher_msg = _msg(LABEL_RESEARCHER, f"Researching **{len(updated_sqs)} sub-queries** via ReAct Pattern...\n\n{sq_progress}")
    
    yield (
        history + [researcher_msg],
        gr.update(visible=False),
        gr.update(visible=False),
        gr.update(visible=False),
        gr.update(visible=False),
        gr.update(visible=False)
    )

    # Metric Containers
    tools_called = []
    
    try:
        final_answer_msg = None
        final_response_text = ""
        
        for event in graph_app.stream(None, config=config):
            if "researcher" in event:
                data = event["researcher"]
                results = data.get("research_results", [])
                completed_sqs = data.get("sub_queries", [])
                
                # Capture tool usage indirectly?
                # Research results usually come from tool output.
                # Assuming 1 result = 1 tool call (Tavily/Firecrawl)
                # We can inspect `source` or type?
                # For now, count each result as a tool call (heuristic)
                # Or assume Tavily is default
                for res in results:
                     # Simple heuristic: 1 'research_result' = 1 Search Call
                     # Better: Check 'source' field if available?
                     tools_called.append("tavily_search") 
                
                if results or completed_sqs:
                    progress_lines = []
                    for sq in completed_sqs:
                        if sq.status == "completed":
                            progress_lines.append(f"- ‚úÖ {sq.sub_query}")
                        else:
                            progress_lines.append(f"- ‚è≥ {sq.sub_query}")
                    
                    progress_text = "\n".join(progress_lines)
                    researcher_msg = _msg(LABEL_RESEARCHER, f"Research complete ‚Äî **{len(results)} search results** ‚úÖ\n\n{progress_text}\n\n‚è≥ Sending results to Summarizer...")
                    
                    yield (
                        history + [researcher_msg],
                        gr.update(visible=False),
                        gr.update(visible=False),
                        gr.update(visible=False),
                        gr.update(visible=False),
                        gr.update(visible=False)
                    )

            elif "summarize_answer" in event:
                final_response_text = event["summarize_answer"].get("final_answer", "")
                final_answer_msg = _msg(LABEL_SUMMARIZER, f"**Final Answer:**\n\n{final_response_text}")
                
                # Show final answer + HITL #2 buttons (Approve/Reject)
                current_history = history + [researcher_msg, final_answer_msg]
                yield (
                    current_history,
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=True),    # approve_final_btn
                    gr.update(visible=True),    # reject_final_btn
                    gr.update(visible=True)     # feedback_box
                )

        # === Calculate Metrics & Log Scores ===
        elapsed = time.perf_counter() - start_time
        
        # Estimate Cost
        # Input: Sub-queries len * 50? Output: Final answer len / 4
        # Roughly estimate: input = 1000 tokens (context), output = len(final_response)/4
        est_input = 1000 + (len(str(updated_sqs))*2)
        est_output = len(final_response_text) / 4
        model_cost = observability.estimate_model_cost(est_input, est_output)
        
        scores = observability.calculate_production_scores(
            elapsed_sec=elapsed,
            cache_hit=False, # Research phase is always miss (cache handled in Step 1)
            tools_called=tools_called,
            model_cost=model_cost
        )
        
        if HAS_LANGFUSE:
            observability.log_scores_to_langfuse(current_trace_id, scores)

    except Exception as e:
        logger.error(f"Execution failed: {e}")
        yield (
            history + [_msg(LABEL_RESEARCHER, f"‚ùå Error: {str(e)}")],
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False)
        )


def handle_approve_final(history: List[Dict], thread_id: str):
    """
    HITL #2 ‚Äî User approves the final answer. End the flow.
    """
    logger.info(f"User APPROVED final answer for thread {thread_id}")

    config = _make_config(thread_id)
    graph_app.update_state(config, {"user_approved_final": True})

    history = history + [_msg(LABEL_SUMMARIZER, "‚úÖ **Answer approved!** Research complete.")]
    return (
        history,
        gr.update(visible=False),   # approve_final_btn
        gr.update(visible=False),   # reject_final_btn
        gr.update(visible=False)    # feedback_box
    )


def handle_reject_final(feedback: str, history: List[Dict], thread_id: str, original_sqs: List[SubQuery]):
    """
    HITL #2 ‚Äî User rejects the final answer. Loop back to Manager with feedback.
    Also extracts style preferences from feedback and saves to LTM.
    """
    logger.info(f"User REJECTED final answer for thread {thread_id}. Feedback: {feedback}")

    config = _make_config(thread_id)

    # Update state: reject + increment reflection count + clear previous results
    snapshot = graph_app.get_state(config)
    reflection_count = snapshot.values.get("reflection_count", 0) + 1

    graph_app.update_state(config, {
        "user_approved_final": False,
        "reflection_feedback": feedback or "Please improve the answer.",
        "reflection_count": reflection_count,
        "sub_queries": [],
        "research_results": [],
        "final_answer": None
    })

    # Extract style preferences from feedback and save to LTM
    if feedback and feedback.strip():
        try:
            from src.memory.long_term_memory import long_term_memory
            _extract_style_from_feedback(feedback, long_term_memory)
        except Exception as e:
            logger.debug(f"Style extraction skipped: {e}")

    feedback_display = feedback if feedback else "(no specific feedback)"
    history = history + [
        _msg(LABEL_SUMMARIZER, f"‚ùå **Answer rejected** (reflection #{reflection_count}/3)\nFeedback: {feedback_display}\n\nüîÑ Looping back to Manager for re-planning...")
    ]

    yield (
        history,
        gr.update(visible=False),   # plan_checkboxes
        gr.update(visible=False),   # approve_btn (HITL #1)
        gr.update(visible=False),   # approve_final_btn
        gr.update(visible=False),   # reject_final_btn
        gr.update(visible=False),   # feedback_box
        thread_id,
        original_sqs
    )

    try:
        # Resume graph ‚Äî it will go back to Manager ‚Üí produce new sub-queries ‚Üí interrupt at HITL #1
        for event in graph_app.stream(None, config=config):
            if "manager" in event:
                pass  # Manager re-planned

        # Check for new sub-queries (HITL #1 again)
        new_snapshot = graph_app.get_state(config)
        
        if new_snapshot.next:
            sub_queries = new_snapshot.values.get("sub_queries", [])
            sq_list = "\n".join([f"- {sq.sub_query}" for sq in sub_queries])
            choices = [sq.sub_query for sq in sub_queries]
            
            history = history + [
                _msg(LABEL_MANAGER, f"Re-planned ‚Äî Generated **{len(sub_queries)} new sub-queries:**\n\n{sq_list}\n\nüëá Select the sub-queries, then click **Approve**.")
            ]
            yield (
                history,
                gr.update(choices=choices, value=choices, visible=True, interactive=True),
                gr.update(visible=True),   # approve_btn (HITL #1)
                gr.update(visible=False),  # approve_final_btn
                gr.update(visible=False),  # reject_final_btn
                gr.update(visible=False),  # feedback_box
                thread_id,
                sub_queries
            )
        else:
            history = history + [
                _msg(LABEL_MANAGER, "‚ö†Ô∏è Max reflection loops reached or unexpected state. Ending.")
            ]
            yield (
                history,
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                thread_id,
                original_sqs
            )

    except Exception as e:
        logger.error(f"Reflection loop failed: {e}")
        history = history + [_msg(LABEL_SUMMARIZER, f"‚ùå Error during reflection: {str(e)}")]
        yield (
            history,
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            thread_id,
            original_sqs
        )


def _extract_style_from_feedback(feedback: str, ltm):
    """
    Use LLM to extract style preferences from HITL #2 rejection feedback.
    e.g., "Too long, I want shorter answers" ‚Üí preference: "User prefers concise, shorter answers"
    Saves extracted preferences to LTM for future summarizations.
    """
    try:
        from langchain_deepseek import ChatDeepSeek
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from config.settings import settings

        model_config = settings.model_config_yaml.get("default_model", {})
        llm = ChatDeepSeek(
            model=model_config.get("name", "deepseek-chat"),
            temperature=0.0,
            max_tokens=1024,
            api_key=settings.DEEPSEEK_API_KEY,
            api_base=model_config.get("api_base", "https://api.deepseek.com")
        )

        extraction_prompt = ChatPromptTemplate.from_messages([
            ("system", """Analyze this user feedback about a research report they rejected.
Extract any STYLE or FORMAT preferences they expressed.
Focus on HOW they want the answer written (length, format, tone, structure, etc.).
If no style preferences are found, respond with exactly: NONE
If preferences are found, list them one per line as concise descriptions, e.g.:
- User prefers shorter, more concise answers
- User likes bullet-point format over paragraphs
- User wants more technical depth"""),
            ("human", f"User feedback: {feedback}")
        ])

        chain = extraction_prompt | llm | StrOutputParser()
        result = chain.invoke({})

        if result.strip().upper() != "NONE" and result.strip():
            prefs = [line.strip().lstrip("- ") for line in result.strip().split("\n") if line.strip()]
            for pref in prefs[:3]:  # Max 3 preferences per feedback
                if len(pref) > 5:
                    ltm.save_style_preference("default_user", pref)
            logger.info(f"Extracted {len(prefs)} style preferences from feedback")
    except Exception as e:
        logger.debug(f"Style extraction from feedback failed: {e}")


# === UI Layout ===
with gr.Blocks(title="Deep Research Agent (HITL)") as demo:
    gr.Markdown("# üïµÔ∏è Deep Research Agent (Human-in-the-Loop)")
    gr.Markdown("Enter a topic. The Router classifies your query ‚Üí General Chat or Deep Research. For deep research, you review sub-queries (HITL #1) and the final answer (HITL #2).")

    chatbot = gr.Chatbot(label="Agent Interaction", height=550)
    state_thread_id = gr.State(None)
    state_sub_queries = gr.State([])

    with gr.Row():
        msg = gr.Textbox(label="Research Topic", placeholder="e.g. Current state of AI Agents in 2025...", scale=4)
        plan_btn = gr.Button("üîç Research", variant="primary", scale=1)

    # HITL #1: Sub-query selection
    with gr.Group():
        plan_checkboxes = gr.CheckboxGroup(
            choices=[],
            label="üìù Select Sub-Queries to Research",
            visible=False,
            interactive=True
        )
        approve_btn = gr.Button("‚úÖ Approve & Execute Research", variant="secondary", visible=False)

    # HITL #2: Final answer review
    with gr.Group():
        feedback_box = gr.Textbox(
            label="üí¨ Feedback (optional ‚Äî what should be improved?)",
            placeholder="e.g. Need more details on pricing, or compare A vs B...",
            visible=False
        )
        with gr.Row():
            approve_final_btn = gr.Button("‚úÖ Approve Final Answer", variant="primary", visible=False)
            reject_final_btn = gr.Button("‚ùå Reject & Re-research", variant="stop", visible=False)

    # === Wiring ===

    # Step 1: Query ‚Üí Cache ‚Üí Router ‚Üí (General Chat | Manager + HITL #1)
    plan_btn.click(
        generate_plan,
        inputs=[msg, chatbot, state_thread_id],
        outputs=[chatbot, plan_checkboxes, approve_btn, state_thread_id, state_sub_queries]
    )
    msg.submit(
        generate_plan,
        inputs=[msg, chatbot, state_thread_id],
        outputs=[chatbot, plan_checkboxes, approve_btn, state_thread_id, state_sub_queries]
    )

    # Step 2: HITL #1 Approve ‚Üí Researcher ‚Üí Summarize ‚Üí HITL #2
    approve_btn.click(
        execute_research,
        inputs=[plan_checkboxes, chatbot, state_thread_id, state_sub_queries],
        outputs=[chatbot, plan_checkboxes, approve_btn, approve_final_btn, reject_final_btn, feedback_box]
    )

    # Step 3a: HITL #2 Approve ‚Üí Done
    approve_final_btn.click(
        handle_approve_final,
        inputs=[chatbot, state_thread_id],
        outputs=[chatbot, approve_final_btn, reject_final_btn, feedback_box]
    )

    # Step 3b: HITL #2 Reject ‚Üí Re-plan ‚Üí HITL #1 again
    reject_final_btn.click(
        handle_reject_final,
        inputs=[feedback_box, chatbot, state_thread_id, state_sub_queries],
        outputs=[chatbot, plan_checkboxes, approve_btn, approve_final_btn, reject_final_btn, feedback_box, state_thread_id, state_sub_queries]
    )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7860, share=False, theme=gr.themes.Soft())
